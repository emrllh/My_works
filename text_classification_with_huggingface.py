# -*- coding: utf-8 -*-
"""Text_classification_with_HuggingFace.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Erv_4m3g8hnQb7-xu2Hx_TgRvERty-T2
"""

!pip install -U datasets



from IPython import get_ipython
from IPython.display import display
from datasets import load_dataset
from datasets import list_datasets

""""
from datasets import list_datasets

all_datasets = list_datasets()
print(f"There are {len(all_datasets)} datasets currently available on the Hub")
print(f"The first 10 are: {all_datasets[:10]}")
"""

from datasets import load_dataset

#so let's load the emotion dataset with the load_dataset() function:
emotions = load_dataset('emotion')

emotions

train_ds=emotions['train']
train_ds

len(train_ds)

#access one of the training
train_ds[0]

train_ds.column_names

#access training features
train_ds.features

train_ds['text'][:3], train_ds['label'][:3]

"""#### If The dataset in not on Hugging Face"""

#  ! character in the preceding shell command, that’s because we’re running the commands in a Jupyter notebook.
# Simply remove the prefix if you want to download and unzip the dataset within a terminal.
#wget is a command-line utility for retrieving files from the web.
dataset_url = "https://huggingface.co/datasets/transformersbook/emotion-train-split/raw/main/train.txt"
!wget {dataset_url}

! head -n 1 train.txt

#in case If My Dataset Is Not on the Hub?
emotions_local = load_dataset('csv', data_files='train.txt', sep=';',
                              names=['text','label'])

emotions_local

dataset_url = "https://huggingface.co/datasets/transformersbook/emotion-train-split/raw/main/train.txt"
emotions_remote = load_dataset("csv", data_files=dataset_url, sep=";",
                               names=["text", "label"])

"""#### From Datasets to DataFrames"""

#set_format() method that allows us to change the output format of the Dataset.
emotions.set_format('pandas')
df = emotions['train'][:]
df.head(5)

# using the int2str() method

def label_int2str(row):
  return emotions['train'].features['label'].int2str(row)


df['label_name'] = df['label'].apply(label_int2str)
df.head(5)

emotions['train'].features['label'].int2str([0,1,2,3,4])

"""#### Looking at the Class Distribution"""

import matplotlib.pyplot as plt
df['label_name'].value_counts(ascending=True).plot.barh()
plt.title("Frequency of Classes")
plt.show()

"""#### How Long Are Our Tweets?"""

# The most tweets are around 15 words long and the longest tweets
df['words per tweet'] =df['text'].str.split().apply(len)
df.boxplot('words per tweet', by='label_name', grid=False, showfliers=False,
           color='black')

plt.suptitle("")
plt.xlabel("")
plt.show()

df['words per tweet'] =df['text'].str.split().apply(len)
df['words per tweet'][:4]

#reset the output format of our dataset since we don't need the DataFrame format anymore:
emotions.reset_format()

"""#### From Text to Tokens
##### Character Tokenization
"""

# Character Tokenition
text = "Tokenizing text is a core task of NLP."

tokonized_text = list(text)

print(tokonized_text)
print(sorted(set(tokonized_text)))

#Change tokonized text to numbers
token2idx ={ch:idx for idx, ch in enumerate(sorted(set(tokonized_text)))}
print(token2idx)

for idx, ch in enumerate(sorted(set(tokonized_text))):
  print(idx,ch)

for token in tokonized_text:
  print(token)

#We can now use token2idx to transform the tokenized text to a list of integers:
input_ids = [token2idx[token] for token in tokonized_text]
print(input_ids)
print(len(token2idx))

# Also panda has get_dummies function
import pandas as pd
categorical_df = pd.DataFrame(
    {"Name": ["Bumblebee", "Optimus Prime", "Megatron"], "Label ID": [0,1,2]})
categorical_df

pd.get_dummies(categorical_df['Name'])

# converting input_ids to a tensor and applying the one_hot() function
import tensorflow as tf
one_hot_encodings = tf.one_hot(input_ids, depth=len(token2idx))
one_hot_encodings.shape

one_hot_encodings[0]

"""#### Word Tokenization"""

#Word Tokenization
text = "Tokenizing text is a core task of NLP."
tokenized_text =text.split() # VS tokonized_text = list(text) for character tokenization
print(tokenized_text)

"""#### Subword Tokenization

The basic idea behind subword tokenization is to combine the best aspects of character and word tokenization. On the one hand, we want to split rare words into smaller units to allow the model to deal with complex words and misspellings. On the other hand, we want to keep frequent words as unique entities so that we can keep the length of our inputs to a manageable size. The main distinguishing feature of subword tokenization (as well as word tokenization) is that it is learned from the pretraining corpus using a mix of statistical rules and algorithms.

There are several subword tokenization algorithms that are commonly used in NLP, one of the main one is WordPiece which is used by the BERT and DistilBERT tokenizers.
https://huggingface.co/learn/llm-course/en/chapter6/6
"""

from transformers import AutoTokenizer #class that allows to quickly load the tokenizer associated with a pretrained model

model_ckpt = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

"""The AutoTokenizer class belongs to a larger set of "auto" classes whose job is to automatically retrieve the model's configuration, pretrained weights, or vocabulary from the name of the checkpoint. This allows you to quickly switch between models, but if you wish to load the specific class manually you can do so as well. For example, we could have loaded the DistilBERT tokenizer as follows:"""

#https://huggingface.co/distilbert/distilbert-base-uncased

from transformers import DistilBertTokenizer


model_ckpt = "distilbert-base-uncased"
distilbert_tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)

#see the tokenizer works
encoded_text = tokenizer(text)
print(encoded_text)

#convert text ids back to tokens
tokens= tokenizer.convert_ids_to_tokens(encoded_text.input_ids)
print(tokens)

print(tokenizer.convert_tokens_to_string(tokens))

#With AutoTokenizer class we can see the vocab size
tokenizer.vocab_size

#model's maximum context size:
tokenizer.model_max_length

tokenizer.model_input_names

"""#### Tokenizing the Whole Dataset

To tokenize the whole corpus, we'll use the `map()` method of our DatasetDict object
"""

def tokenize(batch):
  return tokenizer(batch['text'], padding=True, truncation=True)

print(tokenize(emotions['train'][:2]))

"""By default, the map() method operates individually on every example in the corpus, so setting batched=True will encode the tweets in batches. Because we've set batch_size=None, our tokenize() function will be applied on the full dataset as a single batch. This ensures that the input tensors and attention masks have the same shape globally"""

emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)

#this operation has added new input_ids and attention_mask columns to the dataset:
print(emotions_encoded['train'].column_names)

"""### Training a Text Classifier
We have two options to train such a model on our Twitter dataset

* Feature extraction:: We use the hidden states as features and just train a classifier on them, without modifying the pretrained model.
* Fine-tuning:: We train the whole model end-to-end, which also updates the parameters of the pretrained model.

#### Transformers as Feature Extractors
Using a transformer as a feature extractor is fairly simple.We freeze the body's weights during training and use the hidden states as features for the classifier. The advantage of this approach is that we can quickly train a small or shallow model. Such a model could be a neural classification layer or a method that does not rely on gradients, such as a random forest. This method is especially convenient if GPUs are unavailable, since the hidden states only need to be precomputed once.
"""

#Similar to the AutoTokenizer class, AutoModel has a from_pretrained() method to load the weights of a pretrained model.
from transformers import TFAutoModel
model_ckpt = "distilbert-base-uncased"
tf_model = TFAutoModel.from_pretrained(model_ckpt)

#load pytorch model with from_pt=True

tf_xlmr =TFAutoModel.from_pretrained("xlm-roberta-base", from_pt=True)

"""#### Extracting the last hidden states"""

text = "this is a test"
inputs = tokenizer(text, return_tensors="tf")
print(f"Input tensor shape: {inputs['input_ids'].shape}") #[batch_size, n_tokens]

inputs.items()

#. In our example, the model output is an instance of BaseModelOutput,
#and we can simply access its attributes by name. The current model returns only one attribute, which is the last hidden state,

outputs = tf_model(**inputs)
print(outputs)

#[batch_size, n_tokens, hidden_dim]. In other words, a 768-dimensional vector is returned for each of the 6 input tokens
print(outputs.last_hidden_state.shape)

outputs.last_hidden_state[:,0].shape

#Now that we know how to get the last hidden state for a single string,
#let's do the same thing for the whole dataset by creating a new hidden_state column that stores all these vectors.

def extract_hidden_states(batch):
  inputs = {k: v for k, v in batch.items()
   if k in tokenizer.model_input_names}
  last_hidden_state =tf_model(**inputs).last_hidden_state
  return {'hidden_state':last_hidden_state[:,0].numpy()}

for k, v in emotions_encoded.items():
  print(k,v)

#The map() method requires the processing function to return Python or NumPy objects when we're using batched inputs
#Since our model expects tensors as inputs, the next thing to do is convert the input_ids and attention_mask columns to the "tensor" format

emotions_encoded.set_format(type='tensorflow',columns=["input_ids", "attention_mask", "label"])
emotions_encoded.set_format(type='pytorch',columns=["input_ids", "attention_mask", "label"])

#Note that we did not set batch_size=None in this case, so the default batch_size=1000 is used instead
emotions_hidden = emotions_encoded.map(extract_hidden_states, batched=True)

# applying the +extract_​hid⁠den_​states()+ function has added a new hidden_state column to our dataset
emotions_hidden["train"].column_names, emotions_hidden["train"].shape

np.array(emotions_hidden["train"]['hidden_state']).shape

emotions_hidden["validation"].column_names

emotions_hidden["test"].column_names

"""#### Creating a feature matrix"""

#We will use the hidden states as input features and the labels as targets.
import numpy as np

X_train = np.array(emotions_hidden['train']['hidden_state'])
X_valid = np.array(emotions_hidden['validation']['hidden_state'])
y_train = np.array(emotions_hidden['train']['label'])
y_valid = np.array(emotions_hidden['validation']['label'])
X_train.shape, X_valid.shape
#X_train[0]

"""### Visualizing the training set"""

#Since visualizing the hidden states in 768 dimensions is tricky to say the least, we'll use the powerful UMAP for dimention reduction
#!pip install umap-learn
from umap import UMAP
from sklearn.preprocessing import MinMaxScaler
import pandas as pd

# Scale features to [0,1] range
X_scaled = MinMaxScaler().fit_transform(X_train)
#print(X_scaled[0])

# Initialize and fit UMAP
mapper = UMAP(n_components=2, metric='cosine').fit(X_scaled)

# Create a DataFrame of 2D embeddings
#mapper.embedding_ holds the resulting 2-dimensional coordinates for each data point from original X_scaled data.
df_emb = pd.DataFrame(mapper.embedding_, columns=['X','Y'])
df_emb['label'] = y_train
df_emb.head()

mapper.embedding_, mapper.embedding_.shape

import matplotlib.pyplot as plt
fig, axes = plt.subplots(2,3, figsize=(7,5))
axes = axes.flatten()
cmaps =["Greys", "Blues", "Oranges", "Reds", "Purples", "Greens"]
labels = emotions['train'].features['label'].names
#labels
for i, (label, cmap) in enumerate(zip(labels, cmaps)):
    df_emb_sub = df_emb.query(f"label == {i}")
    axes[i].hexbin(df_emb_sub["X"], df_emb_sub["Y"], cmap=cmap,
                   gridsize=20, linewidths=(0,))
    axes[i].set_title(label)
    axes[i].set_xticks([]), axes[i].set_yticks([])

plt.tight_layout()
plt.show()

""" #### Training a simple classifier

 The model was not trained to know the difference between these emotions. It only learned them implicitly by guessing the masked words in texts.
"""

#Let's use these hidden states to train a logistic regression model with Scikit-Learn. Training such a simple model is fast and does not require a GPU:

from sklearn.linear_model import LogisticRegression

lr_clf = LogisticRegression(max_iter=3000)
lr_clf.fit(X_train, y_train)

lr_clf.score(X_valid,y_valid)

"""Looking at the accuracy, it might appear that our model is just a bit better than random—but since we are dealing with an unbalanced multiclass dataset, it's actually significantly better. We can examine whether our model is any good by comparing it against a simple baseline. In Scikit-Learn there is a DummyClassifier that can be used to build a classifier with simple heuristics such as always choosing the majority class or always drawing a random class. In this case the best-performing heuristic is to always choose the most frequent class, which yields an accuracy of about 35%:"""

from sklearn.dummy import DummyClassifier

dummy_clf = DummyClassifier(strategy= 'most_frequent')
dummy_clf.fit(X_train, y_train)
dummy_clf.score(X_valid, y_valid)

#We can further investigate the performance of the model by looking at the confusion matrix of the classifier
#which tells us the relationship between the true and predicted labels:
from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix

def plot_confusion_matrix(y_preds, y_true, labels):
    cm = confusion_matrix(y_true, y_preds, normalize="true")
    fig, ax = plt.subplots(figsize=(6, 6))
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
    disp.plot(cmap="Blues", values_format=".2f", ax=ax, colorbar=False)
    plt.title("Normalized confusion matrix")
    plt.show()

y_preds = lr_clf.predict(X_valid)
plot_confusion_matrix(y_preds, y_valid, labels)

# Confussions : Love and surprise VS Joy(0.46, 0.37), Fear and Anger VS Sadness(0.17, 0.29)

"""### Fine-Tuning Transformers

We'll be using the Trainer API from Transformers to simplify the training loop.

We use the `AutoModelForSequenceClassification` model instead of `AutoModel`. The difference is that the `AutoModelForSequenceClassification` model has a classification head on top of the pretrained model outputs, which can be easily trained with the base model
"""

emotions_encoded.set_format(type='torch',columns=["text","input_ids", "attention_mask", "label"])
emotions_encoded["train"].column_names

#select the model
import torch

from transformers import AutoModelForSequenceClassification
device = 'cuda' if torch.cuda.is_available() else 'cpu'

model_ckpt = "distilbert-base-uncased"

num_labels =6
pt_model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels).to(device)

# The next step is to define the metrics that we'll use to evaluate our model's performance during fine-tuning.
from sklearn.metrics import accuracy_score,f1_score

def compute_metrics(pred):
  labels = pred.label_ids
  preds = pred.predictions.argmax(-1)
  f1 = f1_score(labels, preds, average='weighted')
  acc = accuracy_score(labels, preds)
  return{'accuracy' : acc, 'f1' : f1}

from huggingface_hub import notebook_login

notebook_login()

from transformers import Trainer, TrainingArguments

batch_size =64
logging_steps = len(emotions_encoded['train']) // batch_size #250
model_name = f'{model_ckpt}-finetuned-emotion'
training_args = TrainingArguments(output_dir=model_name,
                                  num_train_epochs=2,
                                  learning_rate=2e-5,
                                  per_device_train_batch_size=batch_size,
                                  per_device_eval_batch_size=batch_size,
                                  weight_decay=0.01,
                                 eval_strategy='epoch',
                                  disable_tqdm=False,
                                  logging_steps=logging_steps,
                                  push_to_hub=True,
                                  log_level='error')

print(len(emotions_encoded['train']))
logging_steps = len(emotions_encoded['train']) // batch_size
print(logging_steps)

emotions_encoded['train']

from transformers import Trainer

trainer = Trainer(model=pt_model, args=training_args,
                  compute_metrics=compute_metrics,
                  train_dataset=emotions_encoded["train"],
                  eval_dataset=emotions_encoded["validation"],
                  processing_class=tokenizer)
trainer.train();

#saving the model
trainer.save_model('pt_model')

#load saving model

"""We can take a more detailed look at the training metrics by calculating the confusion matrix. To visualize the confusion matrix, we first need to get the predictions on the validation set. The predict() method of the Trainer class returns several useful objects we can use for evaluation:"""

preds_output = trainer.predict(emotions_encoded['validation'])
preds_output.metrics

"""#### With Keras

 Next, we'll convert our datasets into the tf.data.Dataset format. Since we have already padded our tokenized inputs, we can do this easily by applying the to_tf_dataset() method to emotions_encoded:
"""

## The column names to convert to TensorFlow tensors
tekonizer_columns = tokenizer.model_input_names
tekonizer_columns

tf_train_dataset = emotions_encoded['train'].to_tf_dataset(columns=tekonizer_columns, label_cols=['label'], shuffle=True,
                                                           batch_size=batch_size)

tf_eval_dataset = emotions_encoded['validation'].to_tf_dataset(columns=tekonizer_columns, label_cols=['label'], shuffle=False,
                                                               batch_size=batch_size)



tf_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),
                 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                 metrics=tf.metrics.SparseCategoricalAccuracy())

tf_model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=2)

tf_model.history.history

preds_output = tf_model.predict(emotions_encoded["validation"])

tf_model.evaluate(tf_eval_dataset)

model.save_pretrained('tf_model')

#hide_output
repo_id = "tfmodel"
tf_model.push_to_hub(repo_id=repo_id, commit_message="Training completed!")

import torch

# Assuming 'pt_model' is your PyTorch model instance
# Replace 'path/to/your/model.pth' with the desired file path
save_path = 'path/to/your/model.pth'
torch.save(pt_model.state_dict(), save_path)

print(f"Model saved to {save_path}")